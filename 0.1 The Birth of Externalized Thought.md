## The Human Drive to Symbolize
- Before computers, circuits and writing, humans faced the same fundamental problem.
	- Memory is finite, fragile and fallible.
- To survive — to trade, to hunt, to track seasons — we needed a way to externalize thought into the world {tangible form}.
- Computation begins with the human urge to outsource memory and reasoning into symbols.

## Tally Bones
- Tally — a record or count of a number of things.
- Lebombo Bone {~35.000 B.C.E.}
	- Discovered in Swaziland.
	- Carved with 29 notches.
		- Likely a lunar calendar or tally of days.
- Isango Bone {~20.000 B.C.E.}
	- Found in Central Africa.
	- Inscribed with grouped notches.
		- Suggesting prime numbers, multiplication or lunar cycles.
- These were not "calculators" but symbolic representations of memory.
	- Each notch externalized a thought — a count of animals, debts or days.

### Proto-Writing & Tokens
- Etymology
	- From the Greek "prōtos" — "first"
	- From Old English "tacen" —  "symbol"
	-   
- Mesopotamian Clay Tokens {~8.000 B.C.E.}
	- Small clay pieces represented sheep, grain or oil.
	- When stored in sealed clay envelopes {"bullae" — from the singular "bulla"}, they formed an early "database" of transactions.
	- Over time, the impressions on clay evolved into cuneiform writing — computation and writing were born together.
- Tokens show computation as infrastructure, they are state management for society.

### Ancient Civilizations: Computation as Civilization's Engine
- Mesopotamia
	- Invented sexagesimal {base-60} system.
	- Used for time {60 minutes, 60 seconds} and angles {360° circle}.
	- Clay tablets recorded trade, debts and astronomical data.
	- Computation was bureaucratic and astronomical, keeping economies and heavens in order.
- Egypt
	- Papyrus scrolls reveal early unit fractions {1/2, 1/3, 1/4}.
	- Rhind Papyrus {~1650 B.C.E.}
		- Contains algorithms for multiplication, division and geometry.
			- Used in building pyramids and canals.
	- Egyptian scribes acted as programmers of state infrastructure, encoding rules into. repeatable symbolic processes.
- Greece
	- Euclidean Geometry {300 B.C.E.}
		- A formal system of logic + construction rules.
	- Euclid's Algorithm
		- One of the oldest algorithms {for computing the greatest common divisor}
	- Antikythera Mechanism {~100 B.C.E.}
		- A geared bronze device modeling planetary motion — the first known mechanical analog computer
- India & China
	- India
		- Invention of the zero and place-value decimal system {5th century CE}
		- Bakhshali Manuscript
			- Approximations of $\sqrt{2}$ and algebraic manipulation
	- China
		- Rod numerals for place-value calculation
		- Abacus {~200 BCE} became a universal computational device
		- The abacus represents a philosophical shift
			- computation as a tool for anyone, not just scribes or priests, it democratized calculation

### Medieval to Early Renaissance: Computation Becomes Systematic
#### The Transmission of Knowledge
- After the fall of the Western Roman Empire, much ancient mathematical and computational knowledge was preserved and expanded by Arabic scholars.
- They became the bridge between Greek formalism, Indian numerals and the European rediscovery of systematic mathematics
- Al-Khwarizmi {~780=850}
	- Wrote Al-Kitab al-Mukhtasar fi Hisab al-Jabr wal-Muqabala {"The Compendious Book on Calculation by Completion and Balancing"}
	- Introduced systematic algebra
	- His name gave is the word algorithm
- Arabic numerals {originally indian} spread westward}
	- Replaced cumbersome Roman numerals
	- Decimal positional system made arithmetic scalable
- The choice of number system is itself a computational technology. Efficiency of representation shapes efficiency of thought

#### Europe: Manual Computation as Craft
- The Abacus remained the dominant tool until the 16th century
- Fibonacci {Leonardo of Pisa, 1170-1250}
	- In Liber Abaci {1202}, introduced Arabic numerals to Europe
	- Popularized efficient arithmetic, enabling trade and banking
- Calculation schools arose in Renaissance Italy, training merchants in practical arithmetic — an early form of vocational programming education

#### Mechanical Beginnings
- Astronomical Clocks {14-15th centuries}
	- Intricate geared devices not just for timekeeping but also for predicting celestial event
	- First widespread mechanical automata — embodying algorithms in gears.
- Astrolabes
	- Portable devices solving trigonometric problems mechanically
- Physical devices performing repeatable transformations of input into output

#### The Rise of Algorithmic Thinking
- This medieval era marks the shift from ad hoc arithmetic to formal algorithms
- Euclid's logical structure influences scholastic reasoning
- Al-Khwarizml's methods were copied, taught and refined
- Universities began codifying computation as systematic knowledge rather than craft alone

#### The Scientific Revolution & Early Mechanical Automation {17-18th centuries}
- The 17th century was an age of telescopes, microscopes and calculus. Humanity was quantifying the universe at unprecedented scale — and this demanded new tools of computation. Arithmetic by hand and abacus were no longer sufficient
- Computation began its transformation from craft to science

##### Blaise Pascal {1623-1662}: The Pascaline
- built in 1642 to help his father {a tax colelctor}
- Could add and subtract automatically with a system of gears and whells.
- Operated by turning dials, carried digits automatically {like modern arithmetic circuits}
- The Pascaline mechanized part of arithmetic but it did not generalize. It was a special-purpose calculator, not a universal computer

##### Gottfried Wilhelm Leibnix {1646-1716}: THe visionary
- Improved Pascal's design with the Leibniz Wheel {1670}
- Introduced multiplication and division
- Theorized about binary arithmetic
	- Saw that 1s and 0s could encode any number
		- Proposed that all reasoning could be reduced to calculation
- Leibniz envisioned a universal language of symbols and a machine that could manipulate them. This was the seed of universal computation
##### Astronomy, Navigation and Computation
- The need for precise calculation {orbits, navigation, surveying} drove demand for mechanization
- Astronomical tables and logarithmic tables became too large to compute by hand
- Computation became a scientific infrastructure — no longer optional but essential for discovery

##### The 19th Century: Symbolic Generalization & The Birth of Programmability
- Charles Babbage {1791-1871}: Architect of the First Computer
	- Difference Engine {1822}
		- Designed to compute polynomial functions and print mathematical tables
		- Aimed to eliminate human error in navigation and engineering tables
		- Mechanical, massive and partially built — but conceptually revolutionary
	- Analytical Engine {1837}
		- Contained the essential architecture of a modern computer
			- Store {memory}
			- Mill {CPU/processor}
			- Input/Output via punched cards {inspired by Jacquard's loom}
			- Conditional branching {the ability to make decisions}
		- Though never built in his lifetime, the design was Turing-complete in principle
- Babbage moved computation from "calculation" to general-purpose symbolic manipulation

#### Ada Lovelace {1815-18852}: The First Programmer
- Annotated Luigi Menabrea's description of the Analytical Engine
- Wrote the first program {algorithm to compute Bernoulli numbers}
- Understood the machine's potential beyond numbers
	- It could manipulate any symbols if rules were defined
- Anticipated the modern concept of software: separation of hardware and programs
##### The Jacquard Loom {1804}: Inspiration for PRogrammability
- Wove textile patterns using punched cards
- Demonstrated that complex, repeatable processes could be encoded mechanically
- Gave Babbage and Lovelace the metaphor for programmable computation

##### George Boole {1815-1864}: Logic Becomes Algebra
- In "The Laws of Thought" {1854}, formalized Boolean Algebra
	- Operations: AND, OR, NOT
	- Reduced logic to symbolic equations
- This algebra became the mathematical foundation of digital circuit and modern programming

##### Industrial Context
- The 19th century was the age of railways, factories and automation
- Computation began aligning with industrial mechanization
	- From textile weaving {Jacquard} -> to symbolic weaving {Analytical Engine}
	- From hand calculation -> to theoretical universality

#### The 20th Century: Formal Foundations & The Dawn of The Digital Computer
- Alan Turing {1912-1954}: What does it mean to compute?
- Turing Machine {1936}
	- Abstract device with an infinite tape and a read/write head
	- Showed that any computable process can be described as symbol manipulation
	- Defined the boundaries of computability
- The Halting Problem
	- Proved that some problems cannot be solved by any algorithm
	- Introduced the concept of inherent limits to computation
- Turing reframed computation as a concept, not a machine. Any rule-following system is a "computer"

##### John von Neumann {1903-1957}: The Stored-Program Model
- Architect of the von Neumann architecture {1945}
	- Memory stores both data and instructions
	- CPU executes instructions sequentially
	- Still the foundation of most modern computers
- Worked on the EDVAC and influences the first generation of digital computers

##### Claude Shannon {1916-2001}: Logic Meets Circuits
- In his 1937 thesis, showed that Boolean algebra could be implemented with electric circuits {switches, relays}
- Founded Information Theory {1948}:
	- Defined the bit as the unit of information
	- Distinguished between signal {pattern^and meaning {interpretation}
- Shannon's work tuned information into a measurable, engineerable quantity.

##### Early Electronic Computers 
- ENIAC {1945}
	- First large-scale electronic general-purpose computer
		- Programmed manually by rewriting cables
- UNIVAC {1951}
	- First commercial computer
- Colossus {1943}
	- Used by British codebreakers in WWII {cryptanalysis of Lorenz cipher}
		- Zuse's Z3 {1941}
			- First programmable digital computer {Germany}

##### The Rise of Electronics
- Vacuum tubes -> enabled electronic switching
- Transistors {1947}
	- Bell Labs invention that replaced tubes
		- Smaller, faster, more reliable
		- Foundation for modern electronics
- Integrated Circuits {1958}
	- Miniaturized multiple transistors into a single-chip
	- Enabled exponential growth -> Moore's Law {1965}

##### Formal Logic and Computation
- Gödel's Incompleteness Theorems {1931}
	- Limits of formal systems — not all truths can be proven inside the system
- Church-Turing Thesis
	- Anything computable can be computed by a Turing Machine
- Complexity Theory {emerging mid-20th century}
	- Not just what is computable but how efficiently

##### The Modern Era: Microprocessors, Personal Computers and the Internet
###### Microprocessors: Computers on a Chip
- Intel 4004 {1971}
	- First commercially available microprocessor
	- 2.300 transistors
	- Originally designed for calculators
- Intel 8080 {1974} & Motorola 6800
	- Made microcomputers feasible
- Moore's Law {1965}
	- Predicted transistor count would double every 18-24 months
	- Became a self-fulfilling prophecy, guiding the semiconductor industry
- Microprocessors shifted computation from laboratories and corporations into the hands of hobbyists and individuals

###### The Personal Computer Revolution
- Apple II {1977}, IBM PC {1981} and Commodore 64 {1982}
	- Brought computation into homes and schools
- Microsoft & Bill Gates
	- Standardized software through MS-DOS and later Windows.
- Apple & Steve Jobs
	- Focused on design, accessibility and user interface
- Graphical User Interface {GUI}
	- Inspired by Xerox PARC, popularized by Apple Macintosh {1984}
- The PC era reframed computers from specialized machines into personal tools

###### The Internet & Networking
- ARPANET {1969}
	- Military-academic research network, ancestor of the internet
- TCP/IP {1983}
	- Standardized communication protocol, enabling global interconnection
- World Wide Web {1991, Tim Berners-Lee}
	- Combined hypertext + internet
	- Made information globally accessible
- Dot-com boom {1990}
	- Explosion of online commerce and culture
- Computation became networked cognition. The collective external brain of humanity

###### The Era of Ubiquitous Computing
- Mobile Revolution
	- iPhone {2007}
		- defined the modern smartphone
		- Millions of apps transformed computation into a daily companion
- Cloud Computing
	- Amazon Web Services {2006} pioneered scalable infrastructure-as-a-service
	- Shifted computation from local to distributed
- Social Networks
	- Facebook, Twitter, Instagram
	- Computation as a social infrastructure

###### The Rise of Specialized Hardware
- GPU {Graphics Processing Units}
	- Originally for rending graphics, now central do ML/AI
- ASICs & TPUs {google}
	- Custom hardware for machine learning
- Edge computing & IoT
	- Extending computation into every device, from smart watches to industrial services

###### AI, LLMs and The Ongoing Future of Computation: From Symbolic to Statistical
- Early Symbolic AI {1950s-1980s}
	- Logic-based systems
		- AI pioneers like John McCarthy {LISP, 1958} envisioned machines reasoning like humans through symbolic rules
	- Expert Systems {1970s-80s}
		- Encoded domain knowledge as if-then rules
		- Worked well in narrow fields {like medical diagnosis with MYCIN}
		- Failed to scale — brittle, required endless human curation
	- Symbolic reasoning proved computation could simulate logic but not adapt to the messy complexity of the real world
- Machine Learning Rise {1980s-2000s}
	- Statistical learning replaced rules with data
	- Neural networks
		- Inspired by biology but limited by weak hardware and lack of big data
	- Support Vector Machines, decision trees, ensemble methods
		- Offered practical tools for classification and prediction
	- The tipping point
		- Computation + data availability + specialized hardware -> ML became central

###### Deep Learning Revolution {2010s}
- GPUs repurposed for ML training
	- Originally optimized for rendering millions of pixels in parallel, GPUs were perfect for matrix multiplication {the heart of deep learning}
- Big Data
	- Explosion of internet-scale datasets {images, text, audio} gave neural networks the "fuel" they needed
- Breakthroughs
	- AlexNet {2012}
		- CNN that crushed ImageNet competition, triggering deep learning boom
	- Seq2Seq {2014}
		- Enabled translation and generative text
	- GANs {2014}
		- Generative models capable of creating realistic images

###### Why the iPhone Change Everything {2007} — A Case Study in Ubiquity
- Hardware leaps
	- Capacitive multi-touch screen replaced stylus/keyboard -> more natural interface
	- Integration of powerful ARM-based SoC {system-on-chip}
	- Sensors {accelerometer, GPS, camera} fused into one device
- Software paradigm
	- iOS introduced a modern mobile OS with sandboxed apps
	- App Store {2008} -> a software distribution model that scaled globally
- Smartphones shifted computation into an always-on, personal assistant role
	- Not just portable PCs -> but nodes of a global sensor +network mesh

##### Transformers & Large Language Models {2017 -> now}
- Transformers {Vaswani et al., 2017}
	- Architecture that replaced recurrent models with self-attention
		- Allowed parallel training on huge datasets
		- Captured context in language better than any prior model
- GPT series {201-2023}
	- From GPT-1 {proof of concept} -> GPT-4 {multi-modal reasoning}
- LLMs now
	- Trained on trillions of tokens
	- Fine-tuned with reinforcement learning from human feedback {RLHF}
	- Emerging as general-purpose reasoning assistants
